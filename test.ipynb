{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path-RAG: Knowledge-Guided Key Region Retrieval for Open-ended Pathology Visual Question Answering\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/path-rag.png\" width=\"90%\"> <br>\n",
    " \n",
    "  *Accurate diagnosis and prognosis assisted by pathology images are essential for cancer treatment selection and planning. Despite the recent trend of adopting deep-learning approaches for analyzing complex pathology images, they fall short as they often overlook the domain-expert understanding of tissue structure and cell composition. In this work, we focus on a challenging Open-ended Pathology VQA (PathVQA-Open) task and propose a novel framework named Path-RAG, which leverages HistoCartography to retrieve relevant domain knowledge from pathology images and significantly improves performance on PathVQA-Open. Admitting the complexity of pathology image analysis, Path-RAG adopts a human-centered AI approach by retrieving domain knowledge using HistoCartography to select the relevant patches from pathology images. Our experiments suggest that domain guidance can significantly boost the accuracy of LLaVA-Med from 38\\% to 47\\%, with a notable gain of 28\\% for H\\&E-stained pathology images in the PathVQA-Open dataset. For longer-form question and answer pairs, our model consistently achieves significant improvements of 32.5\\% in ARCH-Open PubMed and 30.6\\% in ARCH-Open Books on H\\&E images.*\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "Awais Naeem*, Tianhao Li*, Huang-Ru Liao*, Jiawei Xu*, Aby Mammen Mathew*, Zehao Zhu*, Zhen Tan**, Ajay Jaiswal*, Raffi Salibian*** , Ziniu Hu*** , Tianlong Chen****, Ying Ding*\n",
    "\n",
    "*University of Texas at Austin, USA \\\n",
    "**Arizona State University, USA \\\n",
    "***University of California, Los Angeles, USA \\\n",
    "****Massachusetts Institute of Technology, USA\n",
    "\n",
    "---\n",
    "\n",
    "## Path-RAG Implementation\n",
    "\n",
    "### 1. Clone this repository and navigate to path-rag folder\n",
    "\n",
    "```Shell\n",
    "git clone https://github.com/embedded-robotics/path-rag.git\n",
    "cd path-rag\n",
    "```\n",
    "\n",
    "### 2. Install Package: Create conda environment\n",
    "\n",
    "```Shell\n",
    "conda create -n path-rag python=3.10 -y\n",
    "conda activate path-rag\n",
    "pip install --upgrade pip # enable PEP 660 support for LLaVA-Med\n",
    "```\n",
    "\n",
    "### 3. Download the PathVQA dataset from the following link\n",
    "\n",
    "[PathVQA Dataset](https://github.com/UCSD-AI4H/PathVQA/blob/master/data/README.md)\n",
    "\n",
    "### 4. Clone the HistoCartography tool, setup the model checkpoints in `histocartography/checkpoints` and install the dependencies\n",
    "\n",
    "```Shell\n",
    "git clone https://github.com/BiomedSciAI/histocartography\n",
    "```\n",
    "\n",
    "### 5. Clone the LLaVA-Med repository and install the dependencies\n",
    "\n",
    "```Shell\n",
    "git clone https://github.com/microsoft/LLaVA-Med\n",
    "```\n",
    "\n",
    "### 6. Download the LLaMA-7B model and weights from HuggingFace\n",
    "\n",
    "```Shell\n",
    "python llama_7B_model_weights.py # LLaMA-7B weights/model stored into $HF_HOME (By Default $HF_HOME = ~/.cache/huggingface)\n",
    "```\n",
    "\n",
    "### 7. Download LLaVA-Med delta weights `llava_med_in_text_60k_ckpt2_delta` and `pvqa-9epoch_delta` from `https://github.com/microsoft/LLaVA-Med#model-download`. Put them inside a folder named `model_delta_weights`\n",
    "\n",
    "### 8. Apply the LLaVA-Med delta weights to base LLaMA-7B to come up with the final weights for LLaVA-Med\n",
    "\n",
    "```Shell\n",
    "cd LLaVA-Med\n",
    "```\n",
    "\n",
    "#### LLaVA-Med pre-trained on general biomedicine data\n",
    "\n",
    "```Shell\n",
    "!python3 -m llava.model.apply_delta \\\n",
    "    --base ~/.cache/huggingface/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16 \\\n",
    "    --target ../final_models/llava_med \\\n",
    "    --delta ../model_delta_weights/llava_med_in_text_60k_ckpt2_delta\n",
    "```\n",
    "\n",
    "#### LLaVA-Med fine-tuned on PathVQA\n",
    "\n",
    "```Shell\n",
    "!python -m llava.model.apply_delta \\\n",
    "    --base ~/.cache/huggingface/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16 \\\n",
    "    --target ../final_models/llava_med_pvqa \\\n",
    "    --delta ../model_delta_weights/pvqa-9epoch_delta\n",
    "```\n",
    "\n",
    "```Shell\n",
    "cd ..\n",
    "```\n",
    "\n",
    "### 9. Generate the top patches for open-ended PathVQA images using HistoCartography\n",
    "\n",
    "```Shell\n",
    "python generate_histo_patches.py\n",
    "```\n",
    "\n",
    "### 10. Generate the files for query to be asked for LLaVA-Med for both the images and patches\n",
    "\n",
    "```Shell\n",
    "python generate_llava_med_query.py\n",
    "```\n",
    "\n",
    "### 11. Now we need to generate the answer for all the query files using raw model (`final_models/llava_med`) and fine-tuned model (`final_models/llava_med_pvqa`)\n",
    "\n",
    "```Shell\n",
    "cd LLaVA-Med\n",
    "```\n",
    "\n",
    "#### Raw Model\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med \\\n",
    "    --question-file ../files/query/image_direct.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/raw/answer_image_direct.jsonl\n",
    "```\n",
    "\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med \\\n",
    "    --question-file ../files/query/patch_direct.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/raw/answer_patch_direct.jsonl\n",
    "```\n",
    "\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med \\\n",
    "    --question-file ../files/query/image_description.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/raw/answer_image_description.jsonl\n",
    "```\n",
    "\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med \\\n",
    "    --question-file ../files/query/patch_description.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/raw/answer_patch_description.jsonl\n",
    "```\n",
    "\n",
    "#### Fine-Tuned Model\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med_pvqa \\\n",
    "    --question-file ../files/query/image_direct.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/fine-tuned/answer_image_direct.jsonl\n",
    "```\n",
    "\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med_pvqa \\\n",
    "    --question-file ../files/query/patch_direct.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/fine-tuned/answer_patch_direct.jsonl\n",
    "```\n",
    "\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med_pvqa \\\n",
    "    --question-file ../files/query/image_description.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/fine-tuned/answer_image_description.jsonl\n",
    "```\n",
    "\n",
    "```Shell\n",
    "python llava/eval/model_vqa.py --model-name ../final_models/llava_med_pvqa \\\n",
    "    --question-file ../files/query/patch_description.jsonl \\\n",
    "    --image-folder ../pvqa/images/test \\\n",
    "    --answers-file ../files/answer/fine-tuned/answer_patch_description.jsonl\n",
    "```\n",
    "\n",
    "### 12. Evaluate the results for different use-cases using `recall_calculation.py`\n",
    "\n",
    "(i) Path-RAG w/o GPT: Combine the answer of image + all patches to be the final predicted answer\\\n",
    "(ii) Path-RAG (description): Combine the description of image + all patches. Then involve GPT-4 for reasoning to ge the final predicted answer (See Supplementary Section for Prompts)\\\n",
    "(iii) Path-RAG (answer): Combine the answer of image + all patches. Then involve GPT-4 for reasoning to ge the final predicted answer (See Supplementary Section for Prompts)\n",
    "\n",
    "\n",
    "## ARCH-Open Dataset\n",
    "\n",
    "1. Download the `books_set` and `pubmed_set` of ARCH dataset from  `https://warwick.ac.uk/fac/cross_fac/tia/data/arch`. Store both of these folders in a folder named `arch`. Both `books_set` and `pubmed_set` contains `captions.json` which lists a **caption** and a **UUID**, whereas **UUID** represents the file name in `images` folder and **caption** represents the description of the image.\n",
    "\n",
    "2. Using `captions.json` and `images` folder under `arch/books_set`, run the notebooks `ARCH-OPEN/books_data/synthetic_data_textbook.ipynb` by specifying the OpenAI credentials to generate the question-answer pairs for books set\n",
    "\n",
    "3. Using `captions.json` and `images` folder under `arch/pubmed_set`, run the notebooks `ARCH-OPEN/pubmed_data/synthetic_data_pubmed.ipynb` by specifying the OpenAI credentials to generate the question-answer pairs for pubmed set\n",
    "\n",
    "4. Run the notebook `ARCH-OPEN/synthetic_data_compilation.ipynb` to compile the `pubmed` and `books` question-answer pairs into json files namely `ARCH-OPEN/pubmed_qa_pairs.json` and `ARCH-OPEN/textbook_qa_pairs.json`. These files are already provided to be used directly\n",
    "\n",
    "5. The `pubmed_qa_pairs.json` and `textbook_qa_pairs.json` files contain 5 question-pairs for each pair of `caption` and `uuid` (refers to image name in arch data `arch/pubmed_set/images`, `arch/books_set/images`) in the following format (for both `pubmed_set` and `books_set`):\n",
    "\n",
    "```Shell\n",
    "  {\n",
    "    \"figure_id\": \"00\",\n",
    "    \"letter\": \"A\",\n",
    "    \"caption\": \" A, Spindle cell variant of embryonal rhabdomyosarcoma is characterized by fascicles of eosinophilic spindle cells (B), some of which can show prominent paranuclear vacuolisation, as seen in leiomyosarcoma.\",\n",
    "    \"uuid\": \"890e2e79-ab0a-4a2e-9d62-b0b6b3d43884\",\n",
    "    \"Question_1\": \"What could be the general shape of cells in a spindle cell variant of embryonal rhabdomyosarcoma as seen in the image?\",\n",
    "    \"Answer_1\": \"The cells often present with a spindle-like elongated shape.\",\n",
    "    \"Question_2\": \"What type of structures could be visible in the image indicating the presence of spindle cells?\",\n",
    "    \"Answer_2\": \"Fascicles, or bundles, of cells could be visible in the image, indicating the presence of spindle cells.\",\n",
    "    \"Question_3\": \"Where in the cell would we likely find paranuclear vacuolisation in the image?\",\n",
    "    \"Answer_3\": \"Paranuclear vacuolisation is usually seen around the nucleus area of the cell.\",\n",
    "    \"Question_4\": \"What color might the spindle cells appear in the image?\",\n",
    "    \"Answer_4\": \"The spindle cells may appear eosinophilic, or pinkish-red, under the microscope due to staining.\",\n",
    "    \"Question_5\": \"What visual feature might differentiate spindle cells from leiomyosarcoma cells in the image?\",\n",
    "    \"Answer_5\": \"Spindle cells might show prominent paranuclear vacuolisation, a feature that can differentiate them from leiomyosarcoma cells.\"\n",
    "  }\n",
    "```\n",
    "\n",
    "## Acknolwdgement\n",
    "We would like to acknowledge the following funding supports: NIH OT2OD032581, NIH OTA-21-008, NIH 1OT2OD032742-01, NSF 2333703, NSF 2303038."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
