{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This program will prepare the PVQA dataset to evaluate the self-consistency using LLaVa-Med.\n",
    "\n",
    "1. Read all the open-ended VQA questions\n",
    "2. Prepare a jsonl file such that each question is repeated five times since we need to ask LLaVa-Med five items about a specific question\n",
    "3. Run the LLaVa-Med on the recent file to get five answers about a single question\n",
    "4. Prepare a jsonl file to get the self-consistency output from the model given the question and give answers\n",
    "5. Store the generated answer after self-consistency in a list of dictionary having question, ground truth and predicted answer\n",
    "6. Calculate the recall value using the ground truth and predicted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from recall_calculation import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Test VQA file from PVQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvqa_test_qas_file = \"/data/mn27889/pvqa/qas/test/test_qa.pkl\"\n",
    "\n",
    "with open(pvqa_test_qas_file, 'rb') as file:\n",
    "    pvqa_test_qa = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvqa_qas_open = [qas for qas in pvqa_test_qa if qas['answer'] != 'yes' and qas['answer'] != 'no']\n",
    "# pvqa_ques_open = [qas['question'] for qas in pvqa_qas_open]\n",
    "# pvqa_ans_open = [qas['answer'] for qas in pvqa_qas_open]\n",
    "# pvqa_img_open = [qas['image'] + '.jpg' for qas in pvqa_qas_open]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dictionary format of questions to be put in `jsonl` file for LLaVa-Med. As part of self-consistency, we need to ask a question five times to LLaVa-Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = []\n",
    "idx = 0\n",
    "for qas in pvqa_qas_open:\n",
    "    for _ in range(5):\n",
    "        question.append({\"question_id\": idx, \"image\": qas['image'] + '.jpg', \"text\": qas['question'] + \"\\n<image>\"})\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the question dictionary into a jsonl file with each question separated by a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open.jsonl'\n",
    "\n",
    "# Writing each dictionary as a JSON object on a new line\n",
    "with open(file_path, 'w') as file:\n",
    "    for i in range(0,len(question)):\n",
    "        json_line = json.dumps(question[i])  # Convert dictionary to JSON string\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have around 8 GPU devices on MIATA, we can do parallel processing and divide the questions into 8 jsonl files (no. of GPU devices checked using `nvidia-smi`). Running the LLaVa-Med Evaluation using multiple GPUs (we have 8 GPUs on MIATA - tested using `nvidia-smi`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the split utility to divide `que_pvqa_open.jsonl` file into 8 separete files so that each of the 8 GPU devices can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!split --number=l/8 --additional-suffix=.jsonl pvqa_data/query_files/que_pvqa_open.jsonl  pvqa_data/query_files/que_pvqa_open_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command on terminal to run as a background process by changing GPU_Number as well as the question/answer/log filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_aa.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_aa.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_aa.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_ab.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_ab.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_ab.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=2 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_ac.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_ac.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_ac.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=3 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_ad.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_ad.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_ad.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=4 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_ae.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_ae.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_ae.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=5 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_af.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_af.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_af.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_ag.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_ag.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_ag.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=7 nohup python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_ah.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_ah.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_ah.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine all the individual `answer json` files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat pvqa_data/answer_files/ans_pvqa_open_a*.jsonl > pvqa_data/answer_files/ans_pvqa_open.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the answer file and form the `json` file to check the self-consistency by asking the model to select the best answer out of five for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_pvqa_open_file = \"/data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open.jsonl\"\n",
    "ans_pvqa = []\n",
    "\n",
    "with open(ans_pvqa_open_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        ans_pvqa.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the image information to each dict object in `ans_pvqa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(pvqa_qas_open)):\n",
    "    for j in range(0,5):\n",
    "        ans_pvqa[i*5 + j]['image'] = pvqa_qas_open[i]['image'] + '.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the query list containing ques, five answers and image information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_text = '''Review the user question along with the image and corresponding five responses using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:\n",
    "\n",
    "- Add 1 point if the response is relevant and provides some information related to the pathology, even if it is incomplete or contains some irrelevant content.\n",
    "- Add another point if the response addresses a substantial portion of the user question with some pathology connection, but does not completely resolve the query or provide a direct answer.\n",
    "- Award a third point if the response answers the basic elements of the user question in connection with pathology, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.\n",
    "- Grant a fourth point if the response is clearly written from an AI Assistant perspective, addressing the user question directly and comprehensively, and is medically consistent in terms of pathology\n",
    "- Bestow a fifth point for a response that is impeccably tailored to the user question by an AI Assistant, without extraneous information, reflecting expert knowledge about pathology, and demonstrating a high-quality, engaging, and insightful answer.\n",
    "\n",
    "Question: {ques}\n",
    "\n",
    "Response 1: {ans1}\n",
    "Response 2: {ans2}\n",
    "Response 3: {ans3}\n",
    "Response 4: {ans4}\n",
    "Response 5: {ans5}\n",
    "\n",
    "After examining the user question and each individual respone, conclude with a cumulative score for each response as per the above highlighted scoring system using the format:\n",
    "\n",
    "\"Score 1: <total_points of response 1>, Score 2: <total_points of response 2>, Score 3: <total_points of response 3>, Score 4: <total_points of response 4>, Score 5: <total_points of response 5>\"\n",
    "\n",
    "<image>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sc = []\n",
    "idx = 0\n",
    "\n",
    "\n",
    "for i in range(0, len(ans_pvqa), 5):\n",
    "    # Get the question from first answer\n",
    "    ques = ans_pvqa[i]['prompt']\n",
    "    # Get the image from first answer\n",
    "    img = ans_pvqa[i]['image']\n",
    "    # Get the five answers\n",
    "    ans1 = ans_pvqa[i]['text']\n",
    "    ans2 = ans_pvqa[i+1]['text']\n",
    "    ans3 = ans_pvqa[i+2]['text']\n",
    "    ans4 = ans_pvqa[i+3]['text']\n",
    "    ans5 = ans_pvqa[i+4]['text']\n",
    "    # Prepare the text\n",
    "    query_text = base_text.format(ques=ques, ans1=ans1, ans2=ans2, ans3=ans3, ans4=ans4, ans5=ans5)\n",
    "    query_sc.append({\"question_id\": idx, \"image\": img, \"text\": query_text})\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the query dictionary (self-consistency) into a jsonl file with each query separated by a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_sc.jsonl'\n",
    "\n",
    "# Writing each dictionary as a JSON object on a new line\n",
    "with open(file_path, 'w') as file:\n",
    "    for i in range(0,len(query_sc)):\n",
    "        json_line = json.dumps(query_sc[i])  # Convert dictionary to JSON string\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command on terminal to run as a background process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_projection.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:19<00:00,  9.50s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/data/mn27889/miniconda3/envs/llava-med/lib/python3.10/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:30<00:00,  6.19s/it]\n"
     ]
    }
   ],
   "source": [
    "!python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_test.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_test_llava_pvqa.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:20<00:00, 10.03s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/data/mn27889/miniconda3/envs/llava-med/lib/python3.10/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 5/5 [03:35<00:00, 43.06s/it]\n"
     ]
    }
   ],
   "source": [
    "!python llava/eval/model_vqa.py \\\n",
    "--model-name /data/mn27889/.cache/huggingface/hub/llava_med \\\n",
    "--question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_test.jsonl \\\n",
    "--image-folder /data/mn27889/pvqa/images/test \\\n",
    "--answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_test_llava.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nohup python llava/eval/model_vqa.py \\\n",
    "# --model-name /data/mn27889/.cache/huggingface/hub/llava_med_pvqa \\\n",
    "# --question-file /data/mn27889/LLaVA-Med/pvqa_data/query_files/que_pvqa_open_sc.jsonl \\\n",
    "# --image-folder /data/mn27889/pvqa/images/test \\\n",
    "# --answers-file /data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_sc.jsonl > /data/mn27889/LLaVA-Med/pvqa_data/logs/log_pvqa_open_sc.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the answer file and add the llava_answer to base dictionary of open-ended pvqa questions, images and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_pvqa_sc_open_file = \"/data/mn27889/LLaVA-Med/pvqa_data/answer_files/ans_pvqa_open_sc.jsonl\"\n",
    "ans_pvqa_sc = []\n",
    "\n",
    "with open(ans_pvqa_sc_open_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        ans_pvqa_sc.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvqa_qas_llava_open = []\n",
    "for i in range(0, len(pvqa_qas_open)):\n",
    "    qas = pvqa_qas_open[i]\n",
    "    qas['llava_answer'] = ans_pvqa_sc[i]['text']\n",
    "    pvqa_qas_llava_open.append(qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(pvqa_qas_llava_open, answer_key='answer', prediction_key='llava_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
